# ============================================================================
# Trace Generation Pipeline - Environment Variables
# ============================================================================

# Hugging Face token for downloading datasets (optional but recommended)
HF_TOKEN=your_huggingface_token_here

# vLLM server configuration
VLLM_BASE_URL=http://localhost:8000/v1
MODEL_NAME=openai/gpt-oss-120b

# Modal configuration (set via `modal token set`)
# MODAL_TOKEN_ID=your_modal_token_id
# MODAL_TOKEN_SECRET=your_modal_token_secret

# Generation settings
MAX_TOKENS=16384
TEMPERATURE=0.7
REASONING_LEVEL=high

# Output configuration
OUTPUT_FILE=reasoning_traces.json

# ============================================================================
# Trinity-Mini Serving Configuration
# ============================================================================

# Modal Volume Name
MODAL_VOLUME_NAME=arcee-vol

# Trinity Model Configuration
TRINITY_MODEL_ID=arcee-ai/Trinity-Mini
TRINITY_ADAPTER_PATH_REMOTE=models/trinity-triton-sft-vllm
TRINITY_ADAPTER_PATH_LOCAL=./sft-reasoning/trinity-triton-sft-vllm
SGLANG_ADAPTER_NAME=trinity-reasoning-vllm
