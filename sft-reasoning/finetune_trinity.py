#!/usr/bin/env python3
"""
# SFT Finetuning for Trinity-Mini on Reasoning Traces

This script finetunes the arcee-ai/Trinity-Mini model on verified reasoning traces 
generated by gpt-oss-120b for PyTorch → Triton kernel translation.

## Filtering Criteria:
- correctness == True (kernel output matches reference)
- fast_0 == True (same as correctness - kernel produces correct output)  
- fast_1 and fast_2 are optional (nice to have, but not required)

## Dataset Format:
The traces are stored in `reasoning_traces.json` with the structure:
{
    "pytorch_code": "...",
    "thinking": "<think>reasoning</think>",
    "triton_code": "...",
    "result": {
        "correctness": true/false,
        "speedup": float,
        "fast_0": true/false,  # Must be True
        "fast_1": true/false,  # Optional
        "fast_2": true/false   # Optional
    }
}

## Model:
arcee-ai/Trinity-Mini - 26B MoE with 3B active parameters
Trained for reasoning tasks, 128K context window

Author: Arcee AI
"""

# %% [markdown]
# ## Cell 1: Install Dependencies
# Run this cell first to install all required packages

# %%
# !pip install torch transformers datasets accelerate peft trl bitsandbytes
# !pip install flash-attn --no-build-isolation  # Optional: for faster attention

# %% [markdown]
# ## Cell 2: Imports and Configuration

# %%
import json
import os
from pathlib import Path
from typing import Optional, List, Dict, Any

import torch
from datasets import Dataset, DatasetDict
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    TrainingArguments,
)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from trl import SFTTrainer, SFTConfig

# Configuration
MODEL_NAME = "arcee-ai/Trinity-Mini"
TRACES_FILE = "../reasoning_traces.json"  # Path to generated traces
OUTPUT_DIR = "./trinity-triton-sft"
MAX_SEQ_LENGTH = 4096  # Adjust based on your GPU memory

# Training hyperparameters
LEARNING_RATE = 2e-4
BATCH_SIZE = 1
GRADIENT_ACCUMULATION_STEPS = 8  # Effective batch size = 1 * 8 = 8
NUM_EPOCHS = 3
WARMUP_RATIO = 0.1

# LoRA configuration for efficient finetuning
LORA_R = 64
LORA_ALPHA = 128
LORA_DROPOUT = 0.05

# %% [markdown]
# ## Cell 3: Load and Filter Reasoning Traces
# 
# Filter traces based on the criteria:
# - `correctness` must be True (kernel is correct)
# - `fast_0` must be True (first speed tier achieved)
# - `fast_1` and `fast_2` are optional

# %%
def load_and_filter_traces(
    traces_file: str,
    require_correctness: bool = True,
    require_fast_0: bool = True,
    require_fast_1: bool = False,  # Optional - nice to have
    require_fast_2: bool = False,  # Optional - nice to have
    min_speedup: Optional[float] = None,
) -> List[Dict[str, Any]]:
    """
    Load reasoning traces from JSON and filter based on quality criteria.
    
    Args:
        traces_file: Path to the reasoning_traces.json file
        require_correctness: Filter for correctness == True
        require_fast_0: Filter for fast_0 == True (kernel compiles & is correct)
        require_fast_1: Filter for fast_1 == True (speedup > 1.0)
        require_fast_2: Filter for fast_2 == True (speedup >= 2.0)
        min_speedup: Optional minimum speedup threshold
        
    Returns:
        List of filtered trace dictionaries
    """
    # Load traces
    traces_path = Path(traces_file)
    if not traces_path.exists():
        raise FileNotFoundError(
            f"Traces file not found at {traces_path}. "
            "Please run the orchestrator first to generate traces: "
            "python orchestrator.py"
        )
    
    with open(traces_path, "r") as f:
        traces = json.load(f)
    
    print(f"Loaded {len(traces)} total traces")
    
    # Apply filters
    filtered_traces = []
    stats = {
        "total": len(traces),
        "filtered_correctness": 0,
        "filtered_fast_0": 0,
        "filtered_fast_1": 0,
        "filtered_fast_2": 0,
        "filtered_speedup": 0,
        "passed": 0,
    }
    
    for trace in traces:
        result = trace.get("result", {})
        
        # Check correctness (required)
        if require_correctness and not result.get("correctness", False):
            stats["filtered_correctness"] += 1
            continue
            
        # Check fast_0 (required - kernel compiles and produces correct output)
        if require_fast_0 and not result.get("fast_0", False):
            stats["filtered_fast_0"] += 1
            continue
            
        # Check fast_1 (optional - speedup > 1.0)
        if require_fast_1 and not result.get("fast_1", False):
            stats["filtered_fast_1"] += 1
            continue
            
        # Check fast_2 (optional - speedup >= 2.0)
        if require_fast_2 and not result.get("fast_2", False):
            stats["filtered_fast_2"] += 1
            continue
            
        # Check minimum speedup threshold
        if min_speedup is not None:
            speedup = result.get("speedup", 0.0)
            if speedup < min_speedup:
                stats["filtered_speedup"] += 1
                continue
        
        # Trace passed all filters
        filtered_traces.append(trace)
        stats["passed"] += 1
    
    # Print statistics
    print("\n" + "=" * 60)
    print("FILTERING STATISTICS")
    print("=" * 60)
    print(f"Total traces:              {stats['total']}")
    print(f"Filtered (correctness):    {stats['filtered_correctness']}")
    print(f"Filtered (fast_0):         {stats['filtered_fast_0']}")
    print(f"Filtered (fast_1):         {stats['filtered_fast_1']}")
    print(f"Filtered (fast_2):         {stats['filtered_fast_2']}")
    print(f"Filtered (min_speedup):    {stats['filtered_speedup']}")
    print("-" * 60)
    print(f"Traces passed filters:     {stats['passed']}")
    print(f"Pass rate:                 {stats['passed']/max(stats['total'], 1)*100:.1f}%")
    print("=" * 60 + "\n")
    
    return filtered_traces


# Test loading (will fail if no traces exist yet)
try:
    traces = load_and_filter_traces(
        TRACES_FILE,
        require_correctness=True,
        require_fast_0=True,
        require_fast_1=False,  # Optional
        require_fast_2=False,  # Optional
    )
    print(f"Successfully loaded {len(traces)} filtered traces")
except FileNotFoundError as e:
    print(f"Note: {e}")
    print("Creating sample trace for demonstration...")
    traces = []

# %% [markdown]
# ## Cell 4: Format Traces for SFT Training
#
# Convert traces to the conversational format expected by SFTTrainer:
# - User message: PyTorch code to convert
# - Assistant message: <think>reasoning</think> + Triton code

# %%
def format_trace_for_sft(trace: Dict[str, Any]) -> Dict[str, Any]:
    """
    Format a single trace into the SFT conversational format.
    
    The format follows the pattern:
    User: PyTorch code
    Assistant: <think>reasoning</think><triton>code</triton>
    """
    pytorch_code = trace.get("pytorch_code", "")
    thinking = trace.get("thinking", "")
    triton_code = trace.get("triton_code", "")
    
    # Build the user message
    user_content = f"""Convert the following PyTorch code to an optimized Triton kernel:

```python
{pytorch_code}
```

Generate a complete Triton implementation that produces the same output as the PyTorch code."""

    # Build the assistant message with reasoning and code
    # The thinking should already include <think></think> tags from orchestrator
    # If not, we wrap it
    if thinking and not thinking.strip().startswith("<think>"):
        thinking = f"<think>\n{thinking}\n</think>"
    
    # Wrap triton code in tags if not already
    if triton_code and not triton_code.strip().startswith("<triton>"):
        triton_code = f"<triton>\n{triton_code}\n</triton>"
    
    assistant_content = f"{thinking}\n\n{triton_code}"
    
    # Return in conversational format for SFTTrainer
    return {
        "messages": [
            {"role": "user", "content": user_content},
            {"role": "assistant", "content": assistant_content}
        ]
    }


def prepare_dataset(traces: List[Dict[str, Any]], test_size: float = 0.1) -> DatasetDict:
    """
    Prepare a HuggingFace DatasetDict from filtered traces.
    
    Args:
        traces: List of filtered trace dictionaries
        test_size: Fraction of data to use for validation
        
    Returns:
        DatasetDict with 'train' and 'test' splits
    """
    if not traces:
        # Create sample data for demonstration
        print("No traces available. Creating sample dataset for demonstration.")
        sample_traces = [
            {
                "pytorch_code": """import torch
import torch.nn as nn

class Model(nn.Module):
    def forward(self, x):
        return x * 2

def get_inputs():
    return [torch.randn(1024, 1024).cuda()]""",
                "thinking": """<think>
This is a simple element-wise multiplication by 2.
- Input shape: (1024, 1024) = 1M elements
- Each element is independent, perfect for parallelization
- We'll use a 1D grid with BLOCK_SIZE=1024
- Simple load, multiply, store pattern
</think>""",
                "triton_code": """import torch
import triton
import triton.language as tl

@triton.jit
def mul2_kernel(x_ptr, output_ptr, n_elements, BLOCK_SIZE: tl.constexpr):
    pid = tl.program_id(0)
    block_start = pid * BLOCK_SIZE
    offsets = block_start + tl.arange(0, BLOCK_SIZE)
    mask = offsets < n_elements
    x = tl.load(x_ptr + offsets, mask=mask)
    output = x * 2
    tl.store(output_ptr + offsets, output, mask=mask)

def triton_kernel_wrapper(x):
    output = torch.empty_like(x)
    n_elements = x.numel()
    grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']),)
    mul2_kernel[grid](x, output, n_elements, BLOCK_SIZE=1024)
    return output""",
                "result": {"correctness": True, "fast_0": True, "fast_1": True, "speedup": 1.5}
            }
        ]
        traces = sample_traces
    
    # Format all traces for SFT
    formatted_data = [format_trace_for_sft(trace) for trace in traces]
    
    # Create dataset
    dataset = Dataset.from_list(formatted_data)
    
    # Split into train/test
    if len(dataset) > 1:
        split = dataset.train_test_split(test_size=test_size, seed=42)
        return split
    else:
        # Not enough data for split, use same for both
        return DatasetDict({
            "train": dataset,
            "test": dataset
        })


# Prepare the dataset
dataset = prepare_dataset(traces)
print(f"\nDataset prepared:")
print(f"  Train samples: {len(dataset['train'])}")
print(f"  Test samples:  {len(dataset['test'])}")

# Preview a sample
print("\n" + "=" * 60)
print("SAMPLE TRAINING EXAMPLE")
print("=" * 60)
if len(dataset['train']) > 0:
    sample = dataset['train'][0]
    print(f"User message (truncated):\n{sample['messages'][0]['content'][:500]}...")
    print(f"\nAssistant message (truncated):\n{sample['messages'][1]['content'][:500]}...")

# %% [markdown]
# ## Cell 5: Load Model with Quantization
#
# Load Trinity-Mini with 4-bit quantization for memory efficiency.
# The model is 26B parameters, so quantization is essential for training.

# %%
def load_model_and_tokenizer(
    model_name: str = MODEL_NAME,
    use_4bit: bool = True,
    use_flash_attention: bool = True,
):
    """
    Load the Trinity-Mini model with quantization and LoRA for efficient finetuning.
    
    Args:
        model_name: HuggingFace model name
        use_4bit: Whether to use 4-bit quantization (recommended for 26B model)
        use_flash_attention: Whether to use Flash Attention 2
        
    Returns:
        tuple: (model, tokenizer)
    """
    print(f"Loading tokenizer for {model_name}...")
    tokenizer = AutoTokenizer.from_pretrained(
        model_name,
        trust_remote_code=True,
    )
    
    # Set padding token if not present
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
        tokenizer.pad_token_id = tokenizer.eos_token_id
    
    # Configure quantization
    if use_4bit:
        print("Configuring 4-bit quantization...")
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_use_double_quant=True,  # Nested quantization for more memory savings
            bnb_4bit_quant_type="nf4",  # Normal Float 4 quantization
            bnb_4bit_compute_dtype=torch.bfloat16,  # Compute in bfloat16
        )
    else:
        bnb_config = None
    
    # Model loading kwargs
    model_kwargs = {
        "trust_remote_code": True,
        "torch_dtype": torch.bfloat16,
        "device_map": "auto",
    }
    
    if bnb_config:
        model_kwargs["quantization_config"] = bnb_config
    
    if use_flash_attention:
        try:
            model_kwargs["attn_implementation"] = "flash_attention_2"
            print("Using Flash Attention 2")
        except Exception:
            print("Flash Attention 2 not available, using default attention")
    
    print(f"Loading model {model_name}...")
    print("This may take a few minutes for a 26B parameter model...")
    
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        **model_kwargs,
    )
    
    # Prepare model for k-bit training (required for QLoRA)
    if use_4bit:
        model = prepare_model_for_kbit_training(model)
    
    print(f"Model loaded successfully!")
    print(f"  Model type: {type(model).__name__}")
    print(f"  Device map: {model.hf_device_map if hasattr(model, 'hf_device_map') else 'N/A'}")
    
    return model, tokenizer


# Uncomment to load the model (requires GPU with sufficient VRAM)
# model, tokenizer = load_model_and_tokenizer()

# %% [markdown]
# ## Cell 6: Configure LoRA for Efficient Training
#
# LoRA (Low-Rank Adaptation) allows us to train only a small subset of parameters,
# making it feasible to finetune a 26B model on consumer hardware.

# %%
def create_lora_config(
    r: int = LORA_R,
    lora_alpha: int = LORA_ALPHA,
    lora_dropout: float = LORA_DROPOUT,
    target_modules: Optional[List[str]] = None,
) -> LoraConfig:
    """
    Create LoRA configuration for Trinity-Mini.
    
    Trinity-Mini uses an MoE architecture, so we target:
    - Query, Key, Value projections in attention
    - Gate and up/down projections in MLP
    - Optionally the expert layers (if applicable)
    
    Args:
        r: LoRA rank (higher = more parameters to train)
        lora_alpha: LoRA alpha (scaling factor, typically 2*r)
        lora_dropout: Dropout probability for LoRA layers
        target_modules: List of module names to apply LoRA to
        
    Returns:
        LoraConfig instance
    """
    if target_modules is None:
        # Default targets for typical LLM architectures
        # This should work for most transformer models
        target_modules = [
            "q_proj", "k_proj", "v_proj", "o_proj",  # Attention
            "gate_proj", "up_proj", "down_proj",      # MLP
        ]
    
    lora_config = LoraConfig(
        r=r,
        lora_alpha=lora_alpha,
        lora_dropout=lora_dropout,
        target_modules=target_modules,
        bias="none",
        task_type="CAUSAL_LM",
    )
    
    print("LoRA Configuration:")
    print(f"  Rank (r):          {r}")
    print(f"  Alpha:             {lora_alpha}")
    print(f"  Dropout:           {lora_dropout}")
    print(f"  Target modules:    {target_modules}")
    
    return lora_config


lora_config = create_lora_config()

# %% [markdown]
# ## Cell 7: Configure SFTTrainer
#
# Set up the SFTTrainer with appropriate training arguments.

# %%
def create_training_config(
    output_dir: str = OUTPUT_DIR,
    learning_rate: float = LEARNING_RATE,
    batch_size: int = BATCH_SIZE,
    gradient_accumulation_steps: int = GRADIENT_ACCUMULATION_STEPS,
    num_epochs: int = NUM_EPOCHS,
    max_seq_length: int = MAX_SEQ_LENGTH,
    warmup_ratio: float = WARMUP_RATIO,
) -> SFTConfig:
    """
    Create SFTConfig with optimized settings for Trinity-Mini finetuning.
    
    Returns:
        SFTConfig instance
    """
    config = SFTConfig(
        output_dir=output_dir,
        
        # Training parameters
        num_train_epochs=num_epochs,
        per_device_train_batch_size=batch_size,
        per_device_eval_batch_size=batch_size,
        gradient_accumulation_steps=gradient_accumulation_steps,
        
        # Learning rate
        learning_rate=learning_rate,
        lr_scheduler_type="cosine",
        warmup_ratio=warmup_ratio,
        
        # Optimizer
        optim="adamw_torch_fused" if torch.cuda.is_available() else "adamw_torch",
        weight_decay=0.01,
        max_grad_norm=1.0,
        
        # Sequence settings
        max_seq_length=max_seq_length,
        packing=False,  # Don't pack examples (each kernel trace is independent)
        
        # Memory optimization
        gradient_checkpointing=True,
        gradient_checkpointing_kwargs={"use_reentrant": False},
        
        # Precision
        bf16=torch.cuda.is_bf16_supported() if torch.cuda.is_available() else False,
        fp16=not (torch.cuda.is_bf16_supported() if torch.cuda.is_available() else True),
        
        # Logging
        logging_steps=10,
        logging_first_step=True,
        report_to=["tensorboard"],
        
        # Evaluation
        eval_strategy="steps",
        eval_steps=100,
        save_strategy="steps",
        save_steps=100,
        save_total_limit=3,
        load_best_model_at_end=True,
        
        # Misc
        seed=42,
        dataloader_num_workers=4,
        remove_unused_columns=True,
    )
    
    print("\nTraining Configuration:")
    print(f"  Output directory:     {output_dir}")
    print(f"  Epochs:               {num_epochs}")
    print(f"  Batch size:           {batch_size}")
    print(f"  Gradient accum:       {gradient_accumulation_steps}")
    print(f"  Effective batch:      {batch_size * gradient_accumulation_steps}")
    print(f"  Learning rate:        {learning_rate}")
    print(f"  Max sequence length:  {max_seq_length}")
    print(f"  Gradient checkpoint:  True")
    
    return config


training_config = create_training_config()

# %% [markdown]
# ## Cell 8: Initialize Trainer and Start Training

# %%
def train(
    model,
    tokenizer,
    dataset: DatasetDict,
    training_config: SFTConfig,
    lora_config: LoraConfig,
):
    """
    Initialize SFTTrainer and start training.
    
    Args:
        model: The base model (will have LoRA applied)
        tokenizer: The tokenizer
        dataset: DatasetDict with 'train' and 'test' splits
        training_config: SFTConfig with training parameters
        lora_config: LoraConfig for efficient finetuning
        
    Returns:
        The trained SFTTrainer
    """
    print("\n" + "=" * 60)
    print("INITIALIZING SFT TRAINER")
    print("=" * 60)
    
    trainer = SFTTrainer(
        model=model,
        tokenizer=tokenizer,
        train_dataset=dataset["train"],
        eval_dataset=dataset["test"],
        args=training_config,
        peft_config=lora_config,
    )
    
    # Print trainable parameters
    trainable_params = sum(p.numel() for p in trainer.model.parameters() if p.requires_grad)
    total_params = sum(p.numel() for p in trainer.model.parameters())
    print(f"\nTrainable parameters: {trainable_params:,}")
    print(f"Total parameters:     {total_params:,}")
    print(f"Trainable %:          {100 * trainable_params / total_params:.2f}%")
    
    print("\n" + "=" * 60)
    print("STARTING TRAINING")
    print("=" * 60)
    
    # Train!
    trainer.train()
    
    # Save the final model
    print("\nSaving final model...")
    trainer.save_model()
    
    # Save tokenizer as well
    tokenizer.save_pretrained(training_config.output_dir)
    
    print(f"\nTraining complete! Model saved to: {training_config.output_dir}")
    
    return trainer


# %% [markdown]
# ## Cell 9: Main Training Pipeline
#
# Run this cell to execute the full training pipeline.
# **Note**: This requires a GPU with sufficient VRAM (recommended: A100 40GB or higher)

# %%
def main():
    """
    Main training pipeline.
    
    1. Load and filter reasoning traces
    2. Prepare dataset in SFT format
    3. Load Trinity-Mini with quantization
    4. Apply LoRA for efficient finetuning
    5. Train with SFTTrainer
    6. Save the finetuned model
    """
    print("=" * 60)
    print("TRINITY-MINI FINETUNING PIPELINE")
    print("Finetuning on PyTorch → Triton reasoning traces")
    print("=" * 60)
    
    # Step 1: Load and filter traces
    print("\n[Step 1/5] Loading and filtering traces...")
    try:
        traces = load_and_filter_traces(
            TRACES_FILE,
            require_correctness=True,
            require_fast_0=True,
            require_fast_1=False,  # Optional
            require_fast_2=False,  # Optional
        )
    except FileNotFoundError:
        print("ERROR: No traces file found!")
        print("Please run the orchestrator first to generate traces:")
        print("  cd .. && python orchestrator.py")
        return
    
    if len(traces) < 10:
        print(f"WARNING: Only {len(traces)} traces passed filters.")
        print("Consider generating more traces for better training.")
    
    # Step 2: Prepare dataset
    print("\n[Step 2/5] Preparing dataset...")
    dataset = prepare_dataset(traces)
    
    # Step 3: Load model
    print("\n[Step 3/5] Loading Trinity-Mini model...")
    model, tokenizer = load_model_and_tokenizer()
    
    # Step 4: Configure LoRA
    print("\n[Step 4/5] Configuring LoRA...")
    lora_config = create_lora_config()
    
    # Step 5: Train
    print("\n[Step 5/5] Starting training...")
    training_config = create_training_config()
    trainer = train(model, tokenizer, dataset, training_config, lora_config)
    
    print("\n" + "=" * 60)
    print("TRAINING COMPLETE!")
    print("=" * 60)
    print(f"Model saved to: {OUTPUT_DIR}")
    print("\nTo use the finetuned model:")
    print(f"""
from peft import AutoPeftModelForCausalLM
from transformers import AutoTokenizer

model = AutoPeftModelForCausalLM.from_pretrained("{OUTPUT_DIR}")
tokenizer = AutoTokenizer.from_pretrained("{OUTPUT_DIR}")
""")
    
    return trainer


# Uncomment to run the full training pipeline:
# if __name__ == "__main__":
#     main()

# %% [markdown]
# ## Cell 10: Inference with Finetuned Model
#
# Test the finetuned model on new PyTorch code.

# %%
def inference(
    pytorch_code: str,
    model_path: str = OUTPUT_DIR,
    max_new_tokens: int = 2048,
):
    """
    Run inference with the finetuned model.
    
    Args:
        pytorch_code: PyTorch code to convert to Triton
        model_path: Path to the finetuned model
        max_new_tokens: Maximum tokens to generate
        
    Returns:
        Generated Triton kernel with reasoning
    """
    from peft import AutoPeftModelForCausalLM
    
    print("Loading finetuned model...")
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    model = AutoPeftModelForCausalLM.from_pretrained(
        model_path,
        torch_dtype=torch.bfloat16,
        device_map="auto",
    )
    
    # Format input as conversation
    messages = [
        {
            "role": "user",
            "content": f"""Convert the following PyTorch code to an optimized Triton kernel:

```python
{pytorch_code}
```

Generate a complete Triton implementation that produces the same output as the PyTorch code."""
        }
    ]
    
    # Apply chat template
    prompt = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True,
    )
    
    # Tokenize
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    
    # Generate
    print("Generating Triton kernel...")
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            do_sample=True,
            temperature=0.7,
            top_p=0.95,
            pad_token_id=tokenizer.eos_token_id,
        )
    
    # Decode
    response = tokenizer.decode(outputs[0][inputs["input_ids"].shape[1]:], skip_special_tokens=True)
    
    return response


# Example usage:
# result = inference("""
# import torch
# 
# def softmax(x, dim=-1):
#     x_max = x.max(dim=dim, keepdim=True)[0]
#     exp_x = torch.exp(x - x_max)
#     return exp_x / exp_x.sum(dim=dim, keepdim=True)
# """)
# print(result)

# %% [markdown]
# ## Summary
#
# This script provides a complete pipeline for finetuning Trinity-Mini on reasoning traces:
#
# 1. **Trace Filtering**: Filters traces to only include verified-correct kernels
#    - `correctness == True`
#    - `fast_0 == True`
#
# 2. **Data Formatting**: Converts traces to SFT conversational format
#    - User: PyTorch code
#    - Assistant: `<think>reasoning</think><triton>code</triton>`
#
# 3. **Efficient Training**: Uses QLoRA for memory-efficient finetuning
#    - 4-bit quantization (NF4)
#    - LoRA adapters (only ~1-2% of parameters trained)
#    - Gradient checkpointing
#
# 4. **Inference**: Easy-to-use inference function for new PyTorch code
#
# ### Requirements:
# - GPU with 40GB+ VRAM (A100 recommended)
# - Pre-generated traces from `orchestrator.py`
# - Python 3.10+
#
# ### Next Steps:
# 1. Generate more traces using `orchestrator.py`
# 2. Run this training script on A100/H100 GPU
# 3. Evaluate on held-out KernelBench Level 3+4 problems
# 4. Consider RL refinement (Phase 3) for further improvement

print("Script loaded successfully!")
print("Run main() to start training, or use individual functions as needed.")
